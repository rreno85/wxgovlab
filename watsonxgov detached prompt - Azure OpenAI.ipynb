{"cells":[{"cell_type":"markdown","metadata":{"id":"1431ce28-77ce-443e-9b13-1ca914e8f40d"},"source":["# Lab: Governing an Azure OpenAI Generative AI Model in watsonx.governance 2.x"]},{"cell_type":"markdown","metadata":{"id":"6fc80eab-cd96-42e6-999d-dbe5b864b28b"},"source":["\n","In this lab, you will create a *detached* prompt template asset that references a generative AI model in Azure OpenAI to start governing this model in **watsonx.governance**. You will learn how to perform inference to this external model using the **Openai** Python SDK,  you will then configure some OpenScale monitors to evaluate the model and obtain generative quality and model health metrics.\n","\n","**Notes**\n","\n","- This notebook should be run using with Runtime 22.2 & Python 3.10 or greater runtime environment (e.g.: 3.11, 3.12), if you are viewing this in Watson Studio, and do not see \"Python 3.10/3.11\" in the upper right corner of your screen, please update the runtime now. \n","- This notebook assumes you have **access to an Azure OpenAI account that has the `opeanai-gpt-3.5` model deployed**. If you don't have access to this account, try reserving the `Access to Azure OpenAI GPT 3.5 Model` environment available in IBM's [Techzone](https://techzone.ibm.com/) (as of September 2024).\n","- At some steps in this notebook, you might need to go to the platform and perform some actions using the UI before continuing with the notebook.\n","\n","- If users wish to execute this notebook for task types other than summarization, please consult [this](https://github.com/IBM/watson-openscale-samples/blob/main/IBM%20Cloud/WML/notebooks/watsonx/README.md) document for guidance on evaluating prompt templates for the available task types.\n"]},{"cell_type":"markdown","metadata":{"id":"e98f249b-d866-4a56-b500-e93d3dfb4a81"},"source":["## Prerequisites"]},{"cell_type":"markdown","metadata":{"id":"c93e7764-0439-482a-ac61-835d968dd86a"},"source":["* Service credentials for IBM Watson OpenScale are required.\n","* If you are **not** using Watson Studio to run this notebook, it requires the ID of project in which you want to create the prompt template asset "]},{"cell_type":"markdown","metadata":{"id":"95ba88a7-696b-439b-bd1c-5922e4306188"},"source":["### Contents\n","\n","- [Notebook Setup](#settingup)\n","- [1. Creating the Prompt Template](#ptatsetup)\n","    - [Prompt template](#prompt)\n","- [2. Evaluating Prompt Template in OpenScale](#ptatsetup)\n","    - [Risk evaluations for prompt template asset subscription](#evaluate)\n","    - [Display the Model Risk metrics](#mrmmetric)\n","    - [Display the Generative AI Quality metrics](#genaimetrics)\n","    - [Plot rougel and rougelsum metrics against records](#plotproject)\n","    - [See factsheets information](#factsheetsspace)"]},{"cell_type":"markdown","metadata":{"id":"11000d0d-a3da-463b-97b9-1f766cdac8bd"},"source":["## Setup <a name=\"settingup\"></a>\n","\n","Run the below cell to install the required packages."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f66daba2-fdbd-4d2f-89ae-0dead6f60bd1"},"outputs":[],"source":["!pip install --upgrade datasets==2.10.0 --no-cache | tail -n 1\n","!pip install --upgrade evaluate --no-cache | tail -n 1\n","!pip install --upgrade --extra-index-url https://test.pypi.org/simple/ ibm-aigov-facts-client | tail -n 1\n","!pip install --upgrade \"ibm-watson-openscale>=3.0.4\" | tail -n 1\n","!pip install \"ibm-watson-machine-learning\"\n","!pip install --upgrade matplotlib | tail -n 1\n","!pip install --upgrade pydantic==1.10.11 --no-cache | tail -n 1\n","!pip install --upgrade sacrebleu --no-cache | tail -n 1\n","!pip install --upgrade sacremoses --no-cache | tail -n 1\n","!pip install --upgrade textstat --no-cache | tail -n 1\n","!pip install --upgrade openai rich azure-identity --no-cache | tail -n 1\n","# !pip install --upgrade transformers --no-cache | tail -n 1"]},{"cell_type":"markdown","metadata":{"id":"5068ca17-4f03-42ed-a0fc-596e9a99eb10"},"source":["**Note:** you may need to *restart the kernel* to use the updated packages. You don't need to run the cell above again after restarting"]},{"cell_type":"markdown","metadata":{"id":"bba5b408-c9bf-4b28-99da-079a4b4d7227"},"source":["### Provision services and configure credentials"]},{"cell_type":"markdown","metadata":{"id":"b4adfe25-9475-43bc-b022-6dfc5231db50"},"source":["**ACTION:** Fill the `<YOUR NAME OR NAME INITIALS>` placeholder of the `USER_PREFIX` variable with your name and surname or name initials (e.g.: `USER_PREFIX=\"John Doe\"`)"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"02509910-64aa-4194-aa4c-9740bbea0553"},"outputs":[],"source":["import re\n","\n","# TODO: Fill-in the `USER_PREFIX` variable with your name and surname, or any other unique identifier like your name initials\n","USER_PREFIX = \"<USER PREFIX>\" # (e.g. \"John Doe\" or \"JD\")\n","\n","# Check that your prefix string meets the requirements\n","if re.match(r'[a-z0-9]', USER_PREFIX):\n","    print(\"Thank you! Your prefix '{}' will be prepended to the names of all the assets your create using this notebook.\".format(USER_PREFIX))\n","else:\n","    del USER_PREFIX\n","    raise ValueError(\"Please re-enter prefix in previous cell using only lower case a-z and 0-9\")"]},{"cell_type":"markdown","metadata":{"id":"6ed7518a-4a7c-4e16-80de-fe0fcdfee542"},"source":["Fill-in your platform and Azure credentials:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4dceedfa-1b19-43df-b669-62839d7fb551"},"outputs":[],"source":["import os\n","from rich import print\n","from IPython.display import display, Markdown\n","\n","CPD_URL = \"<EDIT THIS>\"\n","CPD_USERNAME = \"<EDIT THIS>\"\n","CPD_API_KEY = \"<EDIT THIS>\"\n","\n","AZURE_OPENAI_ENDPOINT = \"<EDIT THIS>\"\n","AZURE_OPENAI_DEPLOYMENT_NAME = \"<EDIT THIS>\"\n","AZURE_CLIENT_ID = \"<EDIT THIS>\"\n","AZURE_CLIENT_SECRET = \"<EDIT THIS>\"\n","AZURE_TENANT_ID = \"<EDIT THIS>\"\n","\n","PROJECT_ID = os.environ.get('PROJECT_ID', \"<YOUR_PROJECT_ID>\")\n","print(f\"Your project id is '{PROJECT_ID}'\")"]},{"cell_type":"markdown","metadata":{"id":"eca538bc-4bc9-4866-b0fe-68b6483ddd48"},"source":["### Function to create the access token"]},{"cell_type":"markdown","metadata":{"id":"a89c703a-8f85-4e0f-9aed-a6d806620d83"},"source":["This function generates an IAM access token using the provided credentials. The API calls for creating and scoring prompt template assets utilize the token generated by this function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b002ae27-7cc7-456b-ab9c-a29417f68d06"},"outputs":[],"source":["import requests\n","import urllib3, json  # noqa: E401\n","urllib3.disable_warnings()\n","\n","def generate_access_token():\n","    headers={}\n","    headers[\"Content-Type\"] = \"application/json\"\n","    headers[\"Accept\"] = \"application/json\"\n","    data = {\n","        \"username\":CPD_USERNAME,\n","        \"api_key\":CPD_API_KEY\n","    }\n","    data = json.dumps(data).encode(\"utf-8\")\n","    url = CPD_URL + \"/icp4d-api/v1/authorize\"\n","    response = requests.post(url=url, data=data, headers=headers,verify=False)\n","    response.raise_for_status()\n","    json_data = response.json()\n","    iam_access_token = json_data['token']\n","    print(\"Access token generated succesfully!\")\n","    return iam_access_token\n","\n","iam_access_token = generate_access_token()"]},{"cell_type":"markdown","metadata":{"id":"149f387e-9850-4f12-816c-247cc6ac0c83"},"source":["## 1. Creating the Prompt Template\n","\n","The following cell shows the development of a prompt template used to summarize resumes from job applicants. \n","\n","We will test inference on Azure OpenAI and create a detached prompt template in our project in watsonx thar references the model and prompt.\n","\n","**Action Required : <u>You will need to copy the prompt in section 2, step 14 of the lab instructions to proceed</u>.**"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"40b2226e-e08a-4fb5-8c9f-a5cf1dfb6e54"},"outputs":[],"source":["# TODO: Go back to step 2.14 of the lab instructions and copy the prompt shown there into the cell below\n","# Optional: change the prompt to your liking. Use the {text} placeholder to indicate where the resume text should be filled in\n","PROMPT_TEMPLATE = \"\"\"\n","You will be given a resume. Please summarize the resume in 100 words or less.\n","\n","--- start of text ---\n","{text}\n","--- end of text ---\n","\"\"\".strip()"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"6c23f098-f2e8-458e-844d-2b7f75fa1bdd"},"outputs":[],"source":["import pandas as pd\n","import asyncio\n","from openai import AsyncAzureOpenAI\n","from azure.identity import ClientSecretCredential, get_bearer_token_provider\n","\n","assert not PROMPT_TEMPLATE.startswith('<'), 'Please edit the prompt template according to the lab instructions'\n","\n","def get_azure_token_provider():\n","    default_scope = \"https://cognitiveservices.azure.com/.default\"\n","    credential = ClientSecretCredential(\n","        tenant_id=os.environ.get('AZURE_TENANT_ID', AZURE_TENANT_ID),\n","        client_id=os.environ.get('AZURE_CLIENT_ID' ,AZURE_CLIENT_ID),\n","        client_secret=os.environ.get('AZURE_CLIENT_SECRET', AZURE_CLIENT_SECRET)\n","    )\n","    return get_bearer_token_provider(credential, default_scope)\n","\n","async def summarize_resume(text:str, max_tokens:int=200, token_provider=None):\n","    \"\"\"\n","    This function uses the Azure OpenAI API to summarize the text of the resume given.\n","    Usage: `summary = await summarize('[resume text to summarize]')`\n","    \"\"\"\n","    if token_provider is None:\n","        token_provider = get_azure_token_provider()\n","    client = AsyncAzureOpenAI(\n","        azure_endpoint=os.environ.get('AZURE_OPENAI_ENDPOINT', AZURE_OPENAI_ENDPOINT),\n","        api_version=\"2024-02-15-preview\",\n","        azure_ad_token_provider=token_provider\n","    )\n","    model_response = await client.chat.completions.create(\n","        model=os.environ.get('AZURE_OPENAI_DEPLOYMENT_NAME', AZURE_OPENAI_DEPLOYMENT_NAME),\n","        messages=[{\"role\": \"user\", \"content\": PROMPT_TEMPLATE.format(text=text)}],\n","        max_tokens=max_tokens\n","    )\n","    return model_response.choices[0].message.content\n","\n","async def summarize_batch(resumes:list) -> list:\n","    \"\"\"Summarize all the resumes given\"\"\"\n","    token_provider = get_azure_token_provider()\n","    summaries = await asyncio.gather(\n","        *[summarize_resume(resume, token_provider=token_provider) for resume in resumes]\n","    )\n","    return summaries"]},{"cell_type":"markdown","metadata":{"id":"1860982c-2663-428f-9006-3edfa024d0f1"},"source":["### Load the resume data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"900fd715db90482a8543af0682407871"},"outputs":[],"source":["data = pd.read_csv(\"https://raw.githubusercontent.com/CloudPak-Outcomes/Outcomes-Projects/main/watsonx-governance-l4/data/resume_summarization_test_data.csv\").head(10)\n","print(f\"{len(data)} rows of data loaded\")\n","data.head()"]},{"cell_type":"markdown","metadata":{"id":"77a74110bed64404870ece9ad2957381"},"source":["### Generate the summaries of the resumes\n","\n","**Note:** This might take a while to finish running"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"67f5ec1f4ae2496a8851fe027fb1540c"},"outputs":[],"source":["data['generated_text'] = await summarize_batch(data['Resume'].values)\n","data.head()"]},{"cell_type":"markdown","metadata":{"id":"859fa2ff3bdc44129149499380f8c8ea"},"source":["Display the results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4b5007fda7544422bdb54282888e3900"},"outputs":[],"source":["# you can run this multiple times to show the results from different row samples\n","def display_result(row):\n","    print(f\"[bold]Resume:[/bold]\\n[red]{row.Resume}[/red]\")\n","    print(f\"[bold]AI Generated Summary:[/bold]\\n[blue1]{row.generated_text}[/blue1]\")\n","    print(f\"[bold]Reference (Labeled) Summary:[/bold]\\n[green]{row.Summarization}[/green]\")\n","\n","display_result(data.sample().iloc[0])"]},{"cell_type":"markdown","metadata":{"id":"de9f4870-e1fb-45da-8dd6-6f8d12399009"},"source":["### Create the detached prompt template <a name=\"detached_prompt\"></a>"]},{"cell_type":"markdown","metadata":{"id":"91b5fddd-d6e7-4834-b780-8c23ea0c065b"},"source":["Create a detached prompt template in your project for the summarization task that references the Azure OpenAI model."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"69f88bdc-efb8-4236-bfb1-9a4cdde4bbb0"},"outputs":[],"source":["from ibm_aigov_facts_client import (\n","    AIGovFactsClient, CloudPakforDataConfig,\n","    DetachedPromptTemplate, PromptTemplate\n",")\n","from ibm_aigov_facts_client.utils.enums import Task\n","\n","creds = CloudPakforDataConfig(\n","    service_url=CPD_URL,\n","    username=CPD_USERNAME,\n","    api_key=CPD_API_KEY\n",")\n","facts_client = AIGovFactsClient(\n","    cloud_pak_for_data_configs=creds,\n","    container_id=PROJECT_ID,\n","    container_type=\"project\",\n","    disable_tracing=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"324b7daa-653a-469b-aeb6-b1f0d2c60302"},"outputs":[],"source":["detached_information = DetachedPromptTemplate(\n","    prompt_id=USER_PREFIX+\"-detached-aoai-prompt\",\n","    model_id=f\"azure/{AZURE_OPENAI_DEPLOYMENT_NAME}\",\n","    model_provider=\"Azure OpenAI\",\n","    model_name=\"GPT-3.5-turbo\",\n","    model_url=AZURE_OPENAI_ENDPOINT,\n","    prompt_url=\"prompt_url\",\n","    prompt_additional_info={\"model_owner\": \"Microsoft\", \"model_version\": \"gpt-3.5-turbo-1106\"}\n",")\n","prompt_name = f\"{USER_PREFIX} - Detached prompt for Azure OpenAI GPT-3.5-turbo\"\n","prompt_description = \"A detached prompt for summarization using Azure OpenAI's GPT-3.5-turbo model\"\n","\n","# define parameters for PromptTemplate\n","prompt_template = PromptTemplate(\n","    input=PROMPT_TEMPLATE,\n","    prompt_variables={\"text\": \"\"},\n",")\n","pta_details = facts_client.assets.create_detached_prompt(\n","    model_id=f\"azure/{AZURE_OPENAI_DEPLOYMENT_NAME}\",\n","    task_id=Task.SUMMARIZATION, # 'summarization' task\n","    name=prompt_name,\n","    description=prompt_description,\n","    prompt_details=prompt_template,\n","    detached_information=detached_information\n",")\n","project_pta_id = pta_details.to_dict()[\"asset_id\"]\n","print(f\"Detached Prompt template ID: '{project_pta_id}'\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dd87f51d-301c-476c-b93b-fc99b805bcaf"},"outputs":[],"source":["factsheets_url = f\"{CPD_URL.strip('/')}/wx/prompt-details/{project_pta_id}/factsheet?context=wx&project_id={PROJECT_ID}\"\n","display(Markdown(f\"[Click here to navigate to the published factsheet in the project]({factsheets_url})\"))"]},{"cell_type":"markdown","metadata":{"id":"8f39af31-7049-45b6-a2fd-f9d2874cf481"},"source":["**Action Required: <u>Click the link above to go to the newly published factsheet in your watsonx project</u>**\n","\n","NOTE: At this point, you should <u>**go back to the lab instructions and follow the steps in section 3**</u> before continuing with the notebook."]},{"cell_type":"markdown","metadata":{"id":"7e955a76-f347-4659-a177-c8c0fa7cef2a"},"source":["## 2. Evaluating the prompt with Watson OpenScale <a name=\"ptatsetup\"></a>\n","\n","**NOTE:** <u>Make sure you have started tracking the model in your AI use case before running this part of the notebook (section 3 of the lab instructions).</u> \n","\n","In this section, we will evaluate the prompt template we've created using Watson OpenScale. We will create the different monitors for GenAI models and evaluate the generative quality metrics and model health metrics.\n","\n","Note that we don't go in detail about the different monitors and metrics available in OpenScale for GenAI models. Session 1 of the training should have covered this in more detail. In any case, you can refer to the [OpenScale documentation](https://dataplatform.cloud.ibm.com/docs/content/wsj/model/wos-monitor-gen-quality.html?context=cpdaas) for more information."]},{"cell_type":"markdown","metadata":{"id":"24949ced-6dba-4a57-ae9c-6165a84e92f3"},"source":["### Configure OpenScale"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2b1b2794-3f1f-492e-a2c1-e46f1973fd25"},"outputs":[],"source":["from ibm_cloud_sdk_core.authenticators import IAMAuthenticator, CloudPakForDataAuthenticator\n","\n","from ibm_watson_openscale import *\n","from ibm_watson_openscale.supporting_classes.enums import *\n","from ibm_watson_openscale.supporting_classes import *\n","\n","authenticator = CloudPakForDataAuthenticator(\n","    url=CPD_URL,\n","    username=CPD_USERNAME,\n","    apikey=CPD_API_KEY,\n","    disable_ssl_verification=True\n",")\n","wos_client = APIClient(\n","    service_url=CPD_URL,\n","    authenticator=authenticator,\n","    service_instance_id=None\n",")\n","data_mart_id = wos_client.service_instance_id\n","print(wos_client.version)"]},{"cell_type":"markdown","metadata":{"id":"e92c7fe3-f6f5-48ec-b4a7-819e68f5f257"},"source":["### Openscale instance mapping with the project"]},{"cell_type":"markdown","metadata":{"id":"865b3f9b-fe2c-42ec-995e-ce6f2f3c7330"},"source":["When the authentication is on CPD then we need to add additional step of mapping the project_id/space_id to an OpenScale instance."]},{"cell_type":"code","execution_count":23,"metadata":{"id":"47136904-6ca8-4db4-a0d5-24482a883605"},"outputs":[],"source":["from ibm_watson_openscale.base_classes import ApiRequestFailure\n","\n","\n","try:\n","  wos_client.wos.add_instance_mapping(                \n","    service_instance_id=data_mart_id,\n","    project_id=PROJECT_ID\n","  )\n","except ApiRequestFailure as arf:\n","   if arf.response.status_code == 409:\n","      # Instance mapping already exists. Ignore the error and continue\n","      pass\n","   else:\n","      raise arf"]},{"cell_type":"markdown","metadata":{"id":"6ecc0bc7-bd44-4ce1-b03d-09e36e052161"},"source":["### Setup the prompt template asset in project for evaluation with supported monitor dimensions"]},{"cell_type":"markdown","metadata":{"id":"06ce5dab-2d5d-47ae-b049-30cd663e5eb4"},"source":["The prompt template assets from project is only supported with `development` operational space ID. Running the below cell will create a development type subscription from the prompt template asset created within the project.\n","\n","The available parameters that can be passed for `execute_prompt_setup` function are:\n","\n"," * `prompt_template_asset_id` : Id of prompt template asset for which subscription needs to be created.\n"," * `label_column` :  The name of the column containing the ground truth or actual labels.\n"," * `project_id` : The GUID of the project.\n"," * `space_id` : The GUID of the space.\n"," * `deployment_id` : (optional) The GUID of the deployment.\n"," * `operational_space_id` : The rank of the environment in which the monitoring is happening. Accepted values are `development`, `pre_production`, `production`.\n"," * `problem_type` : (optional) The task type to monitor for the given prompt template asset.\n"," * `classification_type` : The classification type `binary`/`multiclass` applicable only for `classification` problem (task) type.\n"," * `input_data_type` : The input data type.\n"," * `supporting_monitors` : Monitor configuration for the subscription to be created.\n"," * `background_mode` : When `True`, the promt setup operation will be executed in the background"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1ca571ee-5de7-4fc4-8127-c1f0505b6eb0"},"outputs":[],"source":["label_column = \"reference_summary\"\n","operational_space_id = \"development\"\n","problem_type = \"summarization\"\n","input_data_type = \"unstructured_text\"\n","\n","monitors = {\n","    \"generative_ai_quality\": {\n","        \"parameters\": {\n","            \"min_sample_size\": 10,\n","            \"metrics_configuration\": {                    \n","            }\n","        }\n","    }\n","}\n","response = wos_client.wos.execute_prompt_setup(\n","    prompt_template_asset_id=project_pta_id, \n","    project_id=PROJECT_ID,\n","    label_column=label_column,\n","    operational_space_id=operational_space_id, \n","    problem_type=problem_type,\n","    input_data_type=input_data_type, \n","    supporting_monitors=monitors, \n","    background_mode=False\n",")\n","result = response.result\n","result.to_dict()"]},{"cell_type":"markdown","metadata":{"id":"dac0bed8-0422-484e-8305-0fc041c32794"},"source":["With the below cell, users can  read the  prompt setup task and check its status"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"03f033eb-296c-43c3-8881-084a1c70ab1f"},"outputs":[],"source":["response = wos_client.wos.get_prompt_setup( # wos_client.monitor_instances.mrm.get_prompt_setup # if using an older version of facts client\n","    prompt_template_asset_id=project_pta_id,\n","    project_id=PROJECT_ID\n",")\n","\n","result = response.result\n","result_json = result.to_dict()\n","\n","if result_json[\"status\"][\"state\"] == \"FINISHED\":\n","    print(\"Finished prompt setup. The response is {}\".format(result_json))\n","else:\n","    print(\"Prompt setup failed. The response is {}\".format(result_json))"]},{"cell_type":"markdown","metadata":{"id":"85229de2-e3bd-4e06-b8cb-ebc5960dba42"},"source":["### Read required IDs from prompt setup response"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"e211139d-71fa-4dd4-baff-32290233fadc"},"outputs":[],"source":["subscription_id = result_json[\"subscription_id\"]\n","mrm_monitor_instance_id = result_json[\"mrm_monitor_instance_id\"]"]},{"cell_type":"markdown","metadata":{"id":"3a53c1d8-69c8-496e-9aad-b6b3cca4d4e5"},"source":["### Show all the monitor instances of the production subscription\n","The following cell lists the monitors present in the development subscription along with their respective statuses and other details. Please wait for all the monitors to be in active state before proceeding further."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f4550922-0f63-4584-b854-df1331126e91"},"outputs":[],"source":["wos_client.monitor_instances.show(target_target_id=subscription_id)"]},{"cell_type":"markdown","metadata":{"id":"c691231c-b658-4dbe-93b8-dc1dfce92a99"},"source":["### Risk evaluations for PTA subscription <a name=\"evaluate\"></a>"]},{"cell_type":"markdown","metadata":{"id":"8b52a638-24fa-4164-8dc1-b0f9564a50e1"},"source":["### Evaluate the prompt template subscription\n","\n","The following cell will assess the test data with the subscription of the prompt template asset and produce relevant measurements for the configured monitor.\n","<!-- **Note:** If you are running this notebook from Watson studio, you may first need to upload your test data to studio and run code snippet to download feedback data file from project to local directory -->"]},{"cell_type":"markdown","metadata":{"id":"477e8506-9a05-4f05-a4ed-756f5ec6c310"},"source":["The **Risk Assesment** monitor will evaluate your GenAI model for Text Quality and Model Health. You can read more about the available evaluation metrics for GenAI models [by clicking the link here](https://dataplatform.cloud.ibm.com/docs/content/wsj/model/wos-monitors-overview.html?context=wx)\n","\n","> **Note:** For the risk assessment of a development type subscription the user needs to have an evaluation dataset. The risk evaluation function takes the evaluation dataset path as a parameter for evaluation of the configured metric dimensions. If there is a discrepancy between the feature columns in the subscription and the column names in the uploading CSV, users have the option to supply a mapping JSON file to associate the CSV column names with the feature column names in the subscription."]},{"cell_type":"code","execution_count":30,"metadata":{"id":"f55f2942-dade-4832-bd9d-a1e4cce97d15"},"outputs":[],"source":["llm_data = data.copy()\n","llm_data = llm_data[['Resume', 'Summarization', 'generated_text']].rename(columns={\"Resume\":\"text\", 'Summarization': 'reference_summary'})\n","llm_data.to_csv(\"test_data.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"07e04fc7-93c1-4c2c-8d6b-2e4d3c3ad822"},"outputs":[],"source":["test_data_set_name = \"data\"\n","test_data_path = \"test_data.csv\"\n","content_type = \"multipart/form-data\"\n","body = {}\n","response  = wos_client.monitor_instances.mrm.evaluate_risk(\n","    monitor_instance_id=mrm_monitor_instance_id,\n","    test_data_set_name=test_data_set_name, \n","    test_data_path=test_data_path,\n","    content_type=content_type,\n","    body=body,\n","    project_id=PROJECT_ID,\n","    includes_model_output=True,\n","    background_mode=False\n",")"]},{"cell_type":"markdown","metadata":{"id":"addc5251-3931-41b6-a389-6a6c4b433028"},"source":["### Read the risk evaluation response\n","\n","After finishing the risk evaluation, the evaluation results should be available for review:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"565c4fcd-7e77-4de9-9517-bd52edb18d52"},"outputs":[],"source":["response  = wos_client.monitor_instances.mrm.get_risk_evaluation(mrm_monitor_instance_id, project_id=PROJECT_ID)\n","response.result.to_dict()"]},{"cell_type":"markdown","metadata":{"id":"c58e0314-0a54-4dfd-89f9-0c268a57ed69"},"source":["### Display the Model Risk metrics <a name=\"mrmmetric\"></a>\n","\n","Having calculated the measurements for the Foundation Model subscription, the MRM metrics generated for this subscription should now be available for review:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d4bd50cb-1132-43ee-86a1-efa12f5d6a61"},"outputs":[],"source":["wos_client.monitor_instances.show_metrics(monitor_instance_id=mrm_monitor_instance_id, project_id=PROJECT_ID)"]},{"cell_type":"markdown","metadata":{"id":"66a6cefc-39a8-4709-8a31-07250fed5a9d"},"source":["### Display the Generative AI Quality metrics <a name=\"genaimetrics\"></a>\n","\n","[Read the documentation here if you want to know more about each metric](https://dataplatform.cloud.ibm.com/docs/content/wsj/model/wos-monitor-gen-quality.html?context=wx)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9c908f24-fa28-46d7-ba7e-b8807499ea4c"},"outputs":[],"source":["# Get the ID of the generative AI quality monitor\n","monitor_definition_id = \"generative_ai_quality\"\n","result = wos_client.monitor_instances.list(\n","    data_mart_id=data_mart_id,\n","    monitor_definition_id=monitor_definition_id,\n","    target_target_id=subscription_id,\n","    project_id=PROJECT_ID\n",").result\n","result_json = result._to_dict()\n","genaiquality_monitor_id = result_json[\"monitor_instances\"][0][\"metadata\"][\"id\"]\n","genaiquality_monitor_id"]},{"cell_type":"markdown","metadata":{"id":"48886b40-aba5-4249-8468-4d80addf0b24"},"source":["Displaying the GenAIQ monitor metrics generated through the risk evaluation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ca2d329-c876-4a60-a2bd-407a32c2f09a"},"outputs":[],"source":["wos_client.monitor_instances.show_metrics(monitor_instance_id=genaiquality_monitor_id, project_id=PROJECT_ID)"]},{"cell_type":"markdown","metadata":{"id":"4c1ef306-05bf-4783-8364-30a079580f1f"},"source":["### Display record level metrics for Generative AI Quality "]},{"cell_type":"markdown","metadata":{"id":"27d41c59-d679-4545-85a8-9c3ece486686"},"source":["Read the dataset id for generative ai quality dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f5a43371-e9c3-4f07-9aee-5c3bfc041bce"},"outputs":[],"source":["result = wos_client.data_sets.list(\n","    target_target_id=subscription_id,\n","    target_target_type=\"subscription\",\n","    type=\"gen_ai_quality_metrics\"\n",").result\n","\n","genaiq_dataset_id = result.data_sets[0].metadata.id\n","genaiq_dataset_id"]},{"cell_type":"markdown","metadata":{"id":"86cc4427-8ad6-4357-a6c7-a09ac54d5485"},"source":["Displaying record level metrics for generative ai quality – there should be one record per row of data evaluated (10 total)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9e60d542-9530-40f4-9bb4-a961c18fc605"},"outputs":[],"source":["wos_client.data_sets.show_records(data_set_id=genaiq_dataset_id)"]},{"cell_type":"markdown","metadata":{"id":"456a1ba8-ed54-4eea-9ff6-21577d993b31"},"source":["### Plot rougel and rougelsum metrics against records <a name=\"plotproject\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"004589ee-dd7d-4609-91f0-70394bf548ab"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","result = wos_client.data_sets.get_list_of_records(data_set_id = genaiq_dataset_id).result\n","result[\"records\"]\n","x = []\n","y_rougel = []\n","y_rougelsum = []\n","for each in result[\"records\"]:\n","    x.append(each[\"metadata\"][\"id\"][-5:]) # Reading only last 5 characters to fit in the display\n","    y_rougel.append(each[\"entity\"][\"values\"][\"rougel\"])\n","    y_rougelsum.append(each[\"entity\"][\"values\"][\"rougelsum\"])\n","\n","plt.scatter(x, y_rougel, marker='o')\n","\n","# Adding labels and title\n","plt.xlabel(\"X-axis - Record id (last 5 characters)\")\n","plt.ylabel(\"Y-axis - ROUGEL\")\n","plt.title(\"rougel vs record id\")\n","\n","# Display the graph\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"b2e0cdf2-5374-4a1c-aad1-329cc16abe8e"},"source":["Plot rougelsum metrics against records"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d815238e-291e-44a6-8154-f2608e672f52"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","plt.scatter(x, y_rougelsum, marker=\"o\")\n","\n","# Adding labels and title\n","plt.xlabel(\"X-axis - Record ID (last 5 characters)\")\n","plt.ylabel(\"Y-axis - ROUGELSUM\")\n","plt.title(\"rougelsum vs record ID\")\n","\n","# Display the graph\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"95e42399-df9d-4d51-900e-699547c9513e"},"source":["### Navigate to see the published facts in the project <a name=\"factsheetsproject\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ad171990-5f05-4bf9-a864-4ae1a4a7bd8f"},"outputs":[],"source":["factsheets_url = f\"{CPD_URL}/wx/prompt-details/{project_pta_id}/factsheet?context=wx&project_id={PROJECT_ID}\"\n","display(Markdown(f\"[Click here to navigate to the published facts in your watsonx project]({factsheets_url})\"))"]},{"cell_type":"markdown","metadata":{"id":"84062f1f-da4f-4c7b-9910-e18bcd737996"},"source":["**Action Required: <u>Click the link above to visualize the genAI metrics in your factsheet</u>**"]},{"cell_type":"markdown","metadata":{"id":"9b1d6c3b-1bb9-4812-b205-8e9349ee6494"},"source":["### Note: Production Monitoring\n","\n","Recall from session 1, that you set up production monitors for a watsonx.ai model for a RAG use case and manually logged the payload data to OpenScale's datamart, which is a set of tables in the Db2 database connected to the monitoring service. The same process applies to external models, but with an additional caveat:\n","\n","- While for watsonx.ai models deployed in the same environment as your watsonx.governance services, that data is automatically written to the datamart every time you score the model without any further effort or code required. For third-party LLMs (or watsonx.ai LLMs hosted in other environments) that data must be written to the datamart using API calls.\n","\n","\n","Thus, in a production environment, the prompts that you send to the model for inference and the model's response need to be uploaded to OpenScale to continuously trigger evaluations and keep the metrics up-to-date. You can do this by, for example, wrapping the calls to the LLM such that they send payload data to OpenScale or by setting up a separate pipeline that does it in batch (the details will depend on your specific use case). **The image below provides an illustration of how this would work:**\n","\n","![governance-genai-models-drawio](https://dsws-public-data.s3.eu-de.cloud-object-storage.appdomain.cloud/governance-genai-models-drawio.png)\n","\n","- You can refer to your session 1 lab notes if you are interested in setting up production monitoring for GenAI models. Additionally, [this OpenScale sample notebook](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/openscale-apis.html?context=cpdaas) also provides a good reference specific to detached prompt templates.\n","- In the next lab of this session we will go more in detail about setting up production monitoring."]},{"cell_type":"markdown","metadata":{"id":"4030e483-1f39-4c07-b68b-8f2a0a890aac"},"source":["## Congratulations!\n","\n","You have finished the first hands-on lab for this session. Please continue to the next lab and don't forget to share your feedback with the lab instructors."]}],"metadata":{"kernelspec":{"display_name":"Python 3.11","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
